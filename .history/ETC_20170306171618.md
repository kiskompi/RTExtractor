== EXPERIMANTAL ==

# Program működése:
1. Tanulóhalmaz létrehozása:
    1. Felhasználói interakcióval (Selenium WebIDE) használatával csoportosítja a linkeket. Ebből megtanulja az osztályozás szabályait, hogy aztán interakció nélkül tudja kezelni a többi letöltött oldalt.
        * tényleg kell a Selenium a tanuláshoz (vagy csak a végrehajtáshoz)?
    2. lehet hardcoded néhány feature (pl. URL domain része), azok alapján továbbtanulni
2. A felhasználói interakció alapján megtanult csoportosítással a bejárt oldal linkjeit osztályokba rendezi: 
    * SectionRevealButton, 
    * OuterPageHyperlink, 
    * SamePageHyperlink
3. SectionReveaLButton osztályú linkjeit vektorokba rakja, majd sorban meghívja őket.
4. Amikor a SectionReveal linkek elfogytak, az oldal aljára görget. 
    * Ha nem töltődik be új tartalom: 3-as ponthoz.
    * Ha betöltődik, akkor újra vektorba rendezi a inkeket, de csak azokat a SectionReveaLButton típusúakat járja be, amik az előző vektorban nem voltak benne. Ezután vissza a 2-es ponthoz (újra legörget). X (kb. 5) legörgetés után abbahagyja, mert túl nagy lenne a memóriaigény.
5.  Miután a legörgetést abbahagyhja, rámegy egy TraditionalHyperlinkButton típusú linkre, ami ugyanerre az URL-osztályra mutat (pl facebookról csak facebookra).

# Implementáció folyamata:
Mindig működő kis modulokat kell létrehozni, azokat egyemnként tesztelni:
1. Tanulóhalmaz létrehozása Selenium WebIDE segítségével - ez interaktív a user szempontjából.
    1. végigtakkintgatni felvétel módban
    2. ha lehet, Seleniummal Pop Up ablakot generáltatni, ebbe a link katergóriát bevihetővé tenni (sok manuális munka).
    3. a Selenium output fájlját könnyen feldolgozható és felhasználóbarát formára átírni (pl. YAML, LINK / URL, HTML_CLASSES, GROUP_TYPE)
2. Döntési fa létrehozása
    1. implementáció
    2. tanítás
    3. validáció
3. Crawler vagy Crawler API implementálása
4. Extractor API implementálása




== LEGACY ==
#Specifikáció:
Futásidőben generált weblapról automatikusan frissülő szöveggyőjteményt összeállító program.
##A futás menete:
1. Először text inputból vagy user interactionból megszerzi a DOM-elem nevét, amire kattintva a frissítést végzi.
2. Ezután betölti a weblapot, és kinyeri a betöltött oldal tartalmát.
3. Az eltárolt DOM elemre kattintva (ha az nem létezik akkor folyamatosan monitorozva és amint létrejött rákattintva) elindítja az új lekérést. Ez a MAIN LOOP
4. Az eredeti lekérés és az új lekérések tartalmát egyenként szűri, differenciálja és a differenciát elmenti egy fájlba.

#Felmerülő problémák:
1. A jelenlegi rendszerek (pl. Justext) nem alkalmazhatóak, mert a user contentet is boilerplatenek címkézik (nagy code/content ratio, kevés szöveg, kevés stopworddel). Ezt meg kell oldani egy átalakított, vagy új algoritmussal, ami:
    * Nem nézi a szöveg hosszát, vagy ha igen, rövidebbet is elfogad.
    * Nem nézi a code/content ratio-t (FONTOS).
    * STOPWORDöket csak osztályozásra/priorizálásra használja, nem bináris döntésre.
2. Az oldalak más-más módon kérik le az új adatot, ez csak oldalankénti tanulással, vagy felhasználói interakcióval deríthető ki, hogy működik.
    * A Twitter egy linkkel
    * Sok oldal (pl. Facebook) görgetéssel, vagy frissítéssel hozza le az új tartalmat.

