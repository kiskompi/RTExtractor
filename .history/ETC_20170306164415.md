== EXPERIMANTAL ==
1. Felhasználói interakcióval (Selenium WebIDE) használatával csoportosítja a linkeket. Ebből megtanulja az osztályozás szabályait, hogy aztán interakció nélkül tudja kezelni a többi letöltött oldalt.
    1. tényleg kell a Selenium a tanuláshoz (vagy csak a végrehajtáshoz)?
    2. 
2. A felhasználói interakció alapján megtanult csoportosítással a bejárt oldal linkjeit osztályokba rendezi: SectionRevealButton, OuterPageHyperlink, SamePageHyperlink
SectionReveaLButton osztályú linkjeit vektorokba rakja, majd sorban meghívja őket.
3. Amikor a linkek elfogytak, az oldal aljára görget. 
    a. Ha nem töltődik be új tartalom: 3-as ponthoz.
    b. Ha betöltődik, akkor újra vektorba rendezi a inkeket, de csak azokat a SectionReveaLButton típusúakat járja be, amik az előző vektorban nem voltak benne. Ezután vissza a 2-es ponthoz (újra legörget). X (kb. 5) legörgetés után abbahagyja, mert túl nagy lenne a memóriaigény.
4.  Miután a legörgetést abbahagyhja, rámegy egy TraditionalHyperlinkButton típusú linkre, ami ugyanerre az URL-osztályra mutat (pl facebookról csak facebookra).







== LEGACY ==
#Specifikáció:
Futásidőben generált weblapról automatikusan frissülő szöveggyőjteményt összeállító program.
##A futás menete:
1. Először text inputból vagy user interactionból megszerzi a DOM-elem nevét, amire kattintva a frissítést végzi.
2. Ezután betölti a weblapot, és kinyeri a betöltött oldal tartalmát.
3. Az eltárolt DOM elemre kattintva (ha az nem létezik akkor folyamatosan monitorozva és amint létrejött rákattintva) elindítja az új lekérést. Ez a MAIN LOOP
4. Az eredeti lekérés és az új lekérések tartalmát egyenként szűri, differenciálja és a differenciát elmenti egy fájlba.

#Felmerülő problémák:
1. A jelenlegi rendszerek (pl. Justext) nem alkalmazhatóak, mert a user contentet is boilerplatenek címkézik (nagy code/content ratio, kevés szöveg, kevés stopworddel). Ezt meg kell oldani egy átalakított, vagy új algoritmussal, ami:
    * Nem nézi a szöveg hosszát, vagy ha igen, rövidebbet is elfogad.
    * Nem nézi a code/content ratio-t (FONTOS).
    * STOPWORDöket csak osztályozásra/priorizálásra használja, nem bináris döntésre.
2. Az oldalak más-más módon kérik le az új adatot, ez csak oldalankénti tanulással, vagy felhasználói interakcióval deríthető ki, hogy működik.
    * A Twitter egy linkkel
    * Sok oldal (pl. Facebook) görgetéssel, vagy frissítéssel hozza le az új tartalmat.

