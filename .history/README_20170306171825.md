#Bevezetés:
Az internet terjedésével egyre fontosabbá válik az a törekvés, hogy az ott fellelhető információ ne csak a felhasználók, hanem különböző szoftverek által is feldolgozható legyen. Ebből a célból jött létre a 2000-es évek elején a szemantikus web koncepciója, majd emiatt születtek meg a különböző tartalomkinyerési technológiák. A különböző célokra specializált tartalomkinyerő programok célja az, hogy a feladatuk szempontjából releváns információkat a lényegtelen információktól (ún. boilerplate) megtisztítva bányásszák ki a weboldalakból. Ilyen feladat lehet pl. a nyelvészeti célú felhasználás, illetve az információ kis méretű eszközökön jobban megjeleníthetővé tétele és a látássérültek számára készült képernyőolvasó programok által könnyebben feldolgozható formátumba történő alakítása.

A modern tartalomkinyerő rendszerek egyik legnagyobb kihívása, hogy képesek legyenek kibányászni az internet egyre nagyobb hányadát elfoglaló dinamikusan, futási időben generált weblapok releváns tartalmait. Ebbőn a szempontból a web rohamtempóban fejlődve maga mögött hagyta a webbányászatot, mivel annak csúcstechnológiát jelentő épviselői sem képesek ezen adatok teljes mértékű feldolgozására.

Jelen dolgozat célja, hogy röviden bemutassa a dinamikusan generált weblapok működésének alapjait, ismertesse atartalomkinyerés kihívásait ezen a területeén, és végül prezentáljon egy lehetséges megoldást a probléma kezelésére. Ilyen módon szűkíthető lenne a tartalomgenerálás- és kinyerés között utóbbi évtizedben kialakult technológiai rés.


#Részfeladatok:
- [ ] Elmenteni a felhasználó klikkelését, ami megmutatja, melyik link lenyomásával lehet az új adatot behozni. - Selenium WebIDE
- [ ] Képes legyen ezt az inputot megváltoztatni. - Selenium WebIDE szkriptfájljainak kezelése
- [ ] Az inputot (a DOM-elemet, ami aktiválja az adatlekérést) szerkeszthető formában tárolni. - Selenium WebIDE szkriptfájlja
- [ ] Az outputot olvasható formában tárolni. - mi legyen az output?

- [ ] Differencciálja a betöltött HTML-adatokat és csak a különbséget menti el, így csökkentve a redundnciát.
- [ ] Monitorozza az oldalt, ajtomatikus változásokat keresve. Ha az addig láthatatlan, user által adott GET link megjelenik, rákattint (így nem kell az egész oldalt újratölteni).
- [ ] Gépi tanuló API, aminek segítségével az algoritmus felhasználói beavatkozás nélkül tanulhatja a weblap működését.

# Működés:

## Összefoglaló:
A program a Selenium WebIDE szkriptfájljaiból nyeri az inputot. Ezt lekezelve hajtja végre az utasításokat, absztrahálva, hogy ne csak a konkrét elemeket, hanem az ugyanabba az osztályba kerülő elemeket is kezelni tudja. Ehhez kell egy klasszifikátor.

A klasszifikátor működjön úgy, hogy megnézi a klikkelt elemek classait, majd a hasonlóan viselkedőkét (pl amelyik nem töltött be új URL-t) egy csoportba gyűjti a viselkedésének megfelelően. Emellett mindenképp megpróbál lemenni az oldal aljára, akkor is, ha nincs ilyen interakció. Az alulra görgetést viszont nem csinálja a végtelenségig, mert elfogy a memória.


# Program működése:
1. Tanulóhalmaz létrehozása:
    1. Felhasználói interakcióval (Selenium WebIDE) használatával csoportosítja a linkeket. Ebből megtanulja az osztályozás szabályait, hogy aztán interakció nélkül tudja kezelni a többi letöltött oldalt.
        * tényleg kell a Selenium a tanuláshoz (vagy csak a végrehajtáshoz)?
    2. lehet hardcoded néhány feature (pl. URL domain része), azok alapján továbbtanulni
2. A felhasználói interakció alapján megtanult csoportosítással a bejárt oldal linkjeit osztályokba rendezi: 
    * SectionRevealButton, 
    * OuterPageHyperlink, 
    * SamePageHyperlink
3. SectionReveaLButton osztályú linkjeit vektorokba rakja, majd sorban meghívja őket.
4. Amikor a SectionReveal linkek elfogytak, az oldal aljára görget. 
    * Ha nem töltődik be új tartalom: 3-as ponthoz.
    * Ha betöltődik, akkor újra vektorba rendezi a inkeket, de csak azokat a SectionReveaLButton típusúakat járja be, amik az előző vektorban nem voltak benne. Ezután vissza a 2-es ponthoz (újra legörget). X (kb. 5) legörgetés után abbahagyja, mert túl nagy lenne a memóriaigény.
5.  Miután a legörgetést abbahagyhja, rámegy egy TraditionalHyperlinkButton típusú linkre, ami ugyanerre az URL-osztályra mutat (pl facebookról csak facebookra).

# Implementáció folyamata:
- Mindig működő kis modulokat kell létrehozni, azokat egyemnként tesztelni:
1. Tanulóhalmaz létrehozása Selenium WebIDE segítségével - ez interaktív a user szempontjából.
    1. végigtakkintgatni felvétel módban
    2. ha lehet, Seleniummal Pop Up ablakot generáltatni, ebbe a link katergóriát bevihetővé tenni (sok manuális munka).
    3. a Selenium output fájlját könnyen feldolgozható és felhasználóbarát formára átírni (pl. YAML, LINK / URL, HTML\_CLASSES, GROUP\_TYPE)
2. Döntési fa létrehozása
    1. implementáció
    2. tanítás
    3. validáció
3. Crawler vagy Crawler API implementálása
4. Extractor API implementálása


## Tehát:
1. Selenium WebIDE - ebbe jön a felhasználói input, elmenti a szkriptfájlját.
2. Klasszifikátor - a szkriptfájlban lévő DOM elemeket absztrahálja annyira, hogy a weboldal bármelyik hasonló típusú elemére felismerje a böngésző.
3. Végrehajtó - ami a szkriptet a Python Selenium API-val végrehajtatja.

## Lehetséges absztrakciók az interkcióra képes dolgokra:
### SectionRevealButton
    * pl. fb more comment, twitter new posts, Index Mindeközben új hír gombok, ez új szekciót nyit ki ugyanezen az oldalon
### LanguageSelectionButton:
    * ha a gomb megnyomásával a tartalom nem, de a tartalom nyelve megváltozik. pl. euronews
### OuterPageHyperlink:
    * sima link
### SamePageHyperlink:
    * ugyanennek az oldalnak egy másik aloldalára mutató link

## Speciális esetek, amikkel nem kel foglalkozni?
### PopUpSection
    - olyan, felhasználói interakciót igénylő oldalrész, ami blokkolja az oldal tartalmainak elérését. Kódszinten nem probléma, de a felhasználói interakció ami ezt kiiktatja értéktelen a crawlernek, ezért kiszűrendő. (interakció-boilerplate). lehet, hogy csak meg kéne mondani a felhasználóknak, hogy amikor ilyenre kattintsanak, addig állítsák le a felvételt, mert ronthatja a pontosságot/hatékonyságot/sebességet.
### ScrollDownLoader
    - ha lefele görget, akkor új tartalom jön be

Tehát: ismerje meg azokat a linkeket, melyek nem más URL-re irányítanak, hanem ezen az oldalon jelenítenek meg további tartalmakat. 
Ismerje fel azokat a lehetőségeket, melykor az oldal aljára görgetve új adatok töltődnek be. A felhasználói interakciókor a felugró div-ek még bezavarhatnak, de az automatizált crawling futásakor ez már nem szabad.

## Output:
Egy XML-be vagy YAML-ba mentse ki az tanulás eredményét, hogy újrafelhasználható legyen, kb. így:
```XML
<SectionRevealButton>
    <webpage url="www.twitter.com">
        <group type = "SectionRevealButton">
            <class>CLASSNAME1</class>
            <class>CLASSNAME2</class> (azok a classnevek, amik indikálják a tanulás alapján, hogy ebbe a csoportba tartozik)
            <class>CLASSNAME3</class>
        </group>
        <group type = "LanguageSelectionButton">
            <class>CLASSNAME2</class> (azok a classnevek, amik indikálják a tanulás alapján, hogy ebbe a csoportba tartozik)
        </group>
        <group type = "ScrollDownLoader"></group> EZ ÜRES, A LÉTEZÉSE MÁR AZT JELENTI, HOGY HA LEGÖRGETEK, JÖN ÚJ ADAT
    </webpage>
    <webpage url="www.facebook.com">
    </webpage>
    <webpage url="www.euronews.com">
    </webpage>
    <webpage url="www.index.hu">
    </webpage>
</SectionRevealButton>
```
Amit meg kell nézni: van-e bit.ly vagy goo.gl visszafejtő vagy valami hasonló.
## Python libraryk:
    - scikit-learn
    - Pandas

# Implementációs fázisok
##Példa Usecase:
1. Gép valami gépitanulással eldönti, hogy mit kell kattintani és mit nem.
2. Ember hasonlóan...
3. Kettőt összehasonlítva mérjük a gépi tanulás "erejét" az adott feladatra...

##Példa Usecase2:
1. Gép eldönti, hogy hova kell kattintani...
2. Ember átnézi és neki nem tetszik, ekkor belejavít...

##Példa Usecase3:
1. Ember valami minta alapján generáltat valami utasítás listát, hogy ide kell kattintani. (Kvázi Első usecase 2.-pont csak itt az ember is automatizáltan csinálná a dolgokat)
2. A program eszerint működik.

##Továbbiak:

1. "Trollszűrő"
2. youtube commentek
3. euronews multiligual
4. kommentszekciók mutatása

#Kinda docu:
```python
def get_user_action():
    """This function captures the click of the user on the web page and stores it in a file."""
```
```python
def change_user_action():
    """This function changes the file by a new interaction, or manual rewriting.
    Can call get_user_action()"""
```
```python
def difference(old_page_source: list, new_page_source: list) -> str:
    """This function differentiates the two strings given as parameters.
```
```python
def get_refreshment(browser: splinter.Browser) -> list:
    """This function issues a new HTTP request by clicking on the
    "refresh" button saved by the get_refresh_button() function.
```
```python
def get_refresh_button():
    """This function saves the DOM elements of the page responsible
    for getting the new data, marked manually by the
    user. Calls get_user_action() or gets it from text input"""
```
```python
def run_extraction(urladdr: str):
    """The main function, containing the initial GET request, the
    main loop and the output"""
```